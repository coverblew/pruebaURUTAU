Tema 1

Tema 1

Origen y evolución de la inteligencia artificial

Investigación y Gestión de Proyectos en Inteligencia Artificial

Índice

[Esquema 3](#_Toc153878643)

[Ideas clave 4](#_Toc153878644)

[1.1. Introducción y objetivos 4](#_Toc153878645)

[1.2. Definición y origen del concepto inteligencia artificial 4](#_Toc153878646)

[1.3. Fundamentos de la inteligencia artificial 8](#_Toc153878647)

[1.4. Historia de la inteligencia artificial 12](#_Toc153878648)

[1.5. Inteligencia artificial y conceptos relacionados 15](#_Toc153878649)

[1.6. Referencias bibliográficas 24](#_Toc153878650)

[A fondo 26](#_Toc153878651)

[Test 27](#_Toc153878652)

Esquema

![](data:image/jpeg;base64...)

Ideas clave

1.1. Introducción y objetivos

Este tema supone la primera introducción del alumno al concepto de inteligencia artificial, se trata, por tanto, de un tema introductorio con el objetivo genérico de adentrarse en la disciplina. Objetivos más concretos son:

* Ser capaz de enmarcar la evolución de la inteligencia artificial en su contexto histórico.
* Conocer las principales disciplinas o escuelas de la inteligencia artificial.
* Conocer algunos de los términos principales en el ámbito.

1.2. Definición y origen del concepto inteligencia artificial

La inteligencia artificial está presente en todos los ámbitos de nuestra vida. La presencia de este término en los medios de comunicación es frecuente.

La herramienta Google Trends (<https://trends.google.es/trends/>) permite analizar el interés a lo largo del tiempo de un término concreto a partir de las búsquedas realizadas en Google.

Con el término en castellano «inteligencia artificial» se aprecia un interés creciente en el último año (Figura 1).

![](data:image/jpeg;base64...)

Figura 1. Interés a lo largo del tiempo del término «inteligencia artificial».Fuente: <https://trends.google.es/trends/explore?date=2010-01-01%202023-03-31&geo=ES&q=Inteligencia%20Artificial>

Este interés es mucho más evidente si analizamos el término en su versión anglosajona (*artificial intelligence*). Como se ha explicado anteriormente, el inglés es la lengua franca de la ciencia y la técnica, por lo que todos los análisis de este tipo adquieren mayor relevancia si se hacen en este idioma.

La Figura 2 evidencia de forma muy clara la tendencia creciente desde hace una década y el punto de inflexión producido por la aparición de ChatGPT.

![](data:image/jpeg;base64...)

Figura 2. Interés a lo largo del tiempo del término *artificial intelligence*. Fuente: <https://trends.google.es/trends/explore?date=2010-01-01%202023-03-31&geo=ES&q=Artificial%20Intelligence>

Pero ¿qué es la inteligencia artificial?, ¿para qué sirve? Se intentará dar respuesta a estas preguntas en los siguientes apartados.

¿Qué es la inteligencia artificial?

Intuitivamente, el término «inteligencia artificial» hace referencia a **la capacidad que tienen ciertas máquinas de comportarse como humanos.** Sin embargo, esta descripción no puede considerarse una definición, ya que es sumamente ambigua y difusa.

Para llegar a una buena comprensión del término deberíamos primero entender el **significado de inteligencia.** Según la Real Academia Española, este término hace referencia a:

* Capacidad de entender o comprender.
* Capacidad de resolver problemas.
* Conocimiento, comprensión, acto de entender.

Aunque las definiciones parecen claras y evidentes, el espectro se complica según nos adentramos en la interpretación del término. ¿Cuántos tipos de inteligencia hay?, ¿uno, varios?

Howard Gardner (1983) en su famosa obra *Frames of Mind: The Theory of Multiple Intelligences* se atrevió a formular diez tipos de inteligencia:

* Musical.
* Espacial o visual.
* Lingüística-verbal.
* Lógico-matemática.
* Corporal-cinestésica.
* Interpersonal.
* Intrapersonal.
* Naturalista.
* Existencial.
* Otras: categoría en la que entra incluso una inteligencia sexual.

La visión de Gardner no es plenamente aceptada, ya que algunos autores critican que este autor contempla como inteligencias lo que comúnmente se han denominado habilidades o competencias (Geake, 2008). Aun así, **esta teoría ha tenido gran impacto en ámbitos como el educativo,** donde tradicionalmente las pruebas de evaluación se enfocaban a un enfoque puramente lógico matemático y lingüístico.

Son numerosas las instituciones educativas que tratan hoy en día de ampliar la evaluación del estudiante incorporando factores asociados a sus capacidades para la expresión artística o corporal, por poner algunos ejemplos.

Otra discusión asociada al concepto de inteligencia es la exclusividad o no de la **inteligencia como atributo humano.** Sin embargo, los resultados de la experimentación científica han conseguido apartar esta discusión al poner en evidencia que son numerosas las especies animales que consiguen generar nuevas estrategias para adaptarse mejor a las necesidades del entorno y resolver los problemas asociados (Rayner, 1887), y todo ello a pesar de que la experimentación con otras especies animales en este ámbito no es trivial (MacLean, 2014).

Comentado el concepto de inteligencia, queda profundizar en el término que nos ocupaba al inicio. Según la RAE (Real Academia Española), la **inteligencia artificial** es:

«La disciplina científica que se ocupa de crear programas informáticos que ejecutan operaciones comparables a las que realiza la mente humana, como el aprendizaje o el razonamiento lógico».

Otras definiciones del término muy extendidas son las siguientes:

«La automatización de actividades que vinculamos con procesos de pensamiento humano, actividades como la toma de decisiones, resolución de problemas, aprendizaje…» (Bellman, 1978).

«El arte de desarrollar máquinas con capacidad para realizar funciones que cuando son realizadas por personas requieren de inteligencia» (Kurzweil, Richter, Kurzweil y Schneider, 1990).

Como se puede apreciar, la automatización o realización de estas actividades por máquinas (artificial) es una característica común a estas definiciones.

1.3. Fundamentos de la inteligencia artificial

El objetivo de este apartado es identificar algunas disciplinas y conceptos que han contribuido al desarrollo de la inteligencia artificial.

Sin duda alguna, la **filosofía** es una de las disciplinas que más ha contribuido al desarrollo de la inteligencia artificial y lo ha hecho desde diversos enfoques. Las ideas filosóficas han sido básicas para definir los conceptos de «inteligencia», «racional», etc.

Aristóteles (384-322 a. C.) fue el primer autor que intentó definir el conjunto de leyes que rigen el razonamiento. El mallorquín Ramón Llull (1232-1315), patrón de los ingenieros informáticos, dedicó parte de su vida a diseñar una máquina lógica capaz de demostrar una verdad o refutar una mentira. Thomas Hobbes (1588-1679) contemplaba amplias analogías entre el proceso de razonamiento y el proceso de computación numérica.

A partir del siglo XVI aparecen corrientes filosóficas que marcan el debate de la época. Por un lado, **nos encontramos el dualismo frente al materialismo.** El dualismo hace referencia a la existencia de dos componentes independientes entre sí y que marcan nuestras elecciones. Uno de esos componentes de la mente sería el asociado al alma y funciona al margen de las leyes físicas, mientras que el otro se basa exclusivamente en la aplicación de los principios físicos universales. René Descartes (1596-1650) era uno de los firmes defensores de esta corriente.

El materialismo surge como contraposición al dualismo al considerar que todas las operaciones del cerebro se realizan de acuerdo con las leyes de la física que constituyen la mente. Según los materialistas, la conciencia es fruto de la materia. Francis Bacon (1561-1626) y Thomas Hobbes (1588-1679) fueron dos de los máximos exponentes de esta corriente.

Respecto a la procedencia del conocimiento, el **movimiento empírico** iniciado por Bacon establece que todo conocimiento tiene que pasar por la percepción de los sentidos. Como decía John Locke (1632-1704) «nada existe en la mente que no haya pasado antes por los sentidos».

La relación entre conocimiento y acción busca explicar los desencadenantes de las acciones y constituye una cuestión de especial relevancia en la inteligencia artificial. Los **postulados de Aristóteles** son, todavía hoy, una fuente imprescindible a la hora de entender esta relación. Según el filósofo griego las acciones pueden justificarse en base a la conexión lógica entre los objetivos planteados y el conocimiento de la operativa y consecuencia de las acciones aprendidas.

Obviamente, la filosofía no es la única disciplina que ha aportado valor a la evolución de la inteligencia artificial. Las matemáticas, economía, neurociencia, psicología e ingeniería computacional también han realizado indispensables contribuciones a este campo.

Otras contribuciones de la inteligencia artificial

La **matemática** es una de las ciencias que más ha contribuido al desarrollo de la inteligencia artificial. Hoy en día es complicado considerarse experto en inteligencia artificial sin poseer una elevada competencia en el manejo y comprensión de los fundamentos matemáticos relacionados con la lógica, álgebra, cálculo, etc.

Además del concepto de algoritmo trabajado desde la Antigüedad por un amplio abanico de matemáticos, la **lógica matemática** es esencial para permitir el modelado de ciertas conductas. Dos contribuciones fundamentales son el trabajo de **George Boole** (1815-1864) definiendo la lógica proposicional o booleana (se mencionará con más detalle más adelante), y las contribuciones de **Gottlob Frege** (1848-1925), extendiendo la lógica de Boole para incluir objetos y relaciones y dando lugar a la lógica de primer orden que permite la representación del conocimiento.

En 1930, **Kurt Gödel** (1906-1978) da lugar a uno de los mayores hitos de la historia de las matemáticas y la ciencia en sí mismo formulando los famosos **teoremas de incompletitud.** De forma muy resumida y empleando términos sencillos, tomando como base los dos teoremas formulados por Gödel se puede demostrar que existen afirmaciones que no se pueden demostrar verdaderas o falsas empleando la sintaxis y notación algorítmica.

Para muchos autores, el principio de incompletitud pone límite al conocimiento humano al evidenciar la existencia de postulados no demostrables mediante la argumentación matemática.

**Alan Turing** (1912-1954) centró sus investigaciones en lo que sí podía ser demostrado y dio pie a la **máquina de Turing.** El trabajo de Turing fue absolutamente esencial para el desarrollo de la teoría de la computación tal y como lo conocemos actualmente.

![](data:image/jpeg;base64...)

Figura 3. Ejemplo de máquina de Turing empleando herramienta JFlap. Fuente: <https://www.jflap.org/tutorial/turing/one/index.html>

Por último, los trabajos matemáticos relacionados con el estudio de la complejidad algorítmica y el desarrollo de los algoritmos probabilistas también merecen ser mencionados en este resumen.

Respecto a la **economía** pura y las matemáticas aplicadas a la economía, las investigaciones centradas en teoría de la decisión, teoría de juegos e investigación operativa permitieron entender y modelar mejor el concepto de agente inteligente.

La **neurociencia,** en todo lo referente a la estructura neuronal, y la **psicología,** en todo lo referente al análisis del comportamiento, realizan aportaciones destacadas a la evolución de los métodos empleados en la disciplina.

Finalizaremos este apartado hablando de las contribuciones de la **ingeniería informática,** y lo haremos centrándonos exclusivamente en las aportaciones más recientes. En un contexto de gran disposición de datos para entrenar los sistemas inteligentes y mejorar su desempeño, las aportaciones de la ingeniería informática al sector han sido esenciales permitiendo el diseño e implementación de sistemas escalables que permiten realizar en paralelo miles de millones de operaciones posibilitando la creación de aplicaciones que, en muchas ocasiones, compiten de tú a tú con las capacidades humanas (cuando no las superan).

1.4. Historia de la inteligencia artificial

Se resumirá de forma básica en este apartado la evolución de la inteligencia artificial. Se resaltarán algunos hitos destacados en la disciplina.

Aunque el trabajo puro relacionado con la inteligencia artificial comenzó poco después de la Segunda Guerra Mundial, y el término fue formalmente fijado en 1956, existen algunos antecedentes que merece la pena recordar.

En 1912, el genio español **Leonardo Torres Quevedo** (llegó a diseñar el funicular del Niágara) creó una máquina autónoma capaz de jugar al ajedrez.

![](data:image/jpeg;base64...)

Figura 4. Autómata para jugar al ajedrez diseñado por Torres Quevedo. Fuente: <https://divulgadores.com/el-primer-juego-de-ordenador-de-la-historia/>

Constituyó este un hito importante en la época, dando una idea del potencial que podía llegar a adquirir una máquina.

En 1943, **McCulloch y Pitts** proponen una unidad de cálculo que intenta modelar el comportamiento de una neurona «natural», similar a las que constituyen el cerebro humano.

![](data:image/jpeg;base64...)

Figura 5. Unidad de cálculo propuesta por McCulloch y Pitts.

En 1949, **Donald Hebb** formula la regla de Hebb.

En 1950, el gran científico **Alan Turing** publica su *obra Computing Machinery and Intelligence* donde expone su famosa prueba de Turing. Esta prueba se diseñó para proporcionar un mecanismo que permitiese decidir si una máquina tenía un comportamiento «inteligente» o no. La prueba consiste en un humano que contempla dos terminales. En estos terminales hay un humano y un programa que contestan y dialogan con el humano o humanos entrevistadores. Si el humano no es capaz de distinguir entre el humano real y el *software*, entonces podemos decir que este software ha superado la prueba y le podemos considerar inteligente.

En **1950** también aparecen programas informáticos que aplican técnicas de inteligencia artificial como el programa de ajedrez de Arthur Samuel, el solucionador de problemas lógicos de Newell y Simon y la máquina de Gelernter, capaz de deducir problemas de geometría euclídea.

**De 1952 a 1969** se da el período conocido como «Look, Ma, ¡no hands!». Esta época se caracteriza por un gran entusiasmo y expectativa en las aplicaciones de la inteligencia artificial.

En **1956** tiene lugar la Conferencia de Dartmouth (en el Dartmouth College, Estados Unidos). En este encuentro de muchos de los investigadores más influyentes del momento en el área se adoptó oficialmente el término «inteligencia artificial».

En **1965 Robinson** propone un algoritmo capaz de aplicar criterios de razonamiento lógico. Al mismo tiempo, aparece en el MIT el primer *chatbot* denominado Eliza.

En el período comprendido entre **1966 y 1974** se profundiza en el concepto de la complejidad computacional, algo esencial para las futuras evoluciones de la disciplina.

**Entre 1969 y 1979** se desarrollan los primeros sistemas basados en el conocimiento.

En **1974** aparecen los primeros trabajos con coches autónomos, aunque todavía muy primitivos y no muy alejados de coches a control remoto.

**Entre 1980 y 1988** aparecen en el mercado diversos sistemas expertos.

Entre los años **1985 y 1995** se retoma el interés por las redes neuronales.

**A partir de 1988** nuevas soluciones como algoritmos genéticos o *soft computing* contribuyen al enriquecimiento de la disciplina.

**A partir de 1995** se produce la gran explosión de los agentes inteligentes.

En **1997** el ordenador de IBM (*Deep Blue*) derrota a Gary Kasparov en uno de los duelos de ajedrez que más expectación han generado.

En el año **2011,** el programa de IBM Watson derrota al campeón (humano) del conocido juego *Jeopardy.*

En el año **2015,** el *software* desarrollado por Google, DeepMind AlphaGo, derrota al campeón mundial de *Go.*

En el año **2019,** DeepMind AlphaStar consigue ser gran maestro en la liga mundial de jugadores de *Starcraft II,* ganando el 98 % de las partidas.

No todo han sido éxitos en la historia de la inteligencia artificial, **los períodos que abarcan los años 1974-1980 y 1987-1993 son conocidos como los inviernos de la inteligencia artificial.** Estas franjas de tiempo se caracterizan por una gran desilusión en las posibilidades de esta disciplina y descenso del número de aportaciones.

Aunque no constituye un hito puramente científico, merece la pena mencionar la **aparición de las tres leyes de la robótica de Asimov** (aunque también hay quien asocia la coautoría de estas leyes con John W. Campbell). La aparición de estas leyes propició interesantes debates sobre la evolución de la inteligencia artificial. Las tres leyes de la robótica de Asimov establecen los ejes fundamentes que deberían guiar la conducta de un robot. Son las siguientes:

![](data:image/jpeg;base64...)

Figura 6. Leyes de la robótica de Asimov.

Las **leyes de Asimov** guardan relación con los retos éticos que afronta la inteligencia artificial y que serán tratados en próximos temas.

1.5. Inteligencia artificial y conceptos relacionados

Inteligencia artificial es un concepto muy amplio y genérico. Por ello es útil y necesario precisar con mayor exactitud algunas de las aplicaciones o campos de actuación de la inteligencia artificial más referenciados.

Robótica

Comencemos con el concepto de robótica. Según la RAE, un robot es una «máquina o ingenio electrónico programable, capaz de manipular objetos y realizar operaciones antes reservadas solo a las personas». El término robot proviene de su correspondiente anglosajón (con la misma grafía), que proviene a su vez de la voz checa *robota*, que significa ‘trabajo, prestación personal’.

El significado original del término tiene sentido porque los robots se concibieron como máquinas destinadas a ayudar al ser humano a realizar las tareas más pesadas, estando ahora totalmente introducidos en casi la totalidad de las cadenas de producción.

Robótica, por tanto, es la ciencia que estudia el diseño y construcción de máquinas autónomas capaz de realizar tareas de forma inteligente, resolviendo problemas y adaptándose a los cambios que suceden en el entorno.

A la hora de hablar de robótica, haremos referencia con relativa frecuencia a Sophia por el nivel de interlocución con los humanos que demuestra. Sophia es producto de Hanson Robotics.

![](data:image/jpeg;base64...)

Figura 7. Sophia, robot creado por Hanson Robotics. Fuente: <https://www.hansonrobotics.com/wp-content/uploads/2018/09/gds_Sophia_D058.jpg>

Sin embargo, no es Sophia el tipo de robot susceptible de estar más presente en nuestras casas, los robots aspiradoras o robots juguetes son ejemplos más cercanos a nuestros hogares.

Sistemas expertos

Una de las ramas más conocidas de la inteligencia artificial es la de los sistemas expertos. Un sistema experto **intenta acumular el conocimiento existente en un ámbito concreto (por ejemplo, el área de atención primaria de medicina) y aplicarlo a la toma de decisiones empleando procesos de razonamiento lógico.** El desarrollo de sistemas expertos se potenció especialmente a partir de los años 70. Hoy en día disponemos de sistemas expertos que ayudan a tomar mejores decisiones en el ámbito clínico, transporte aéreo, seguridad, gestión de empresas, etc.

El desarrollo de un sistema experto suele exigir el modelado del conocimiento asociado para que la máquina pueda tenerlo en consideración. Para esta tarea son especialmente indicadas las **ontologías.** Una ontología pretende representar el conocimiento existente, así como las propiedades y relaciones entre los distintos conceptos de forma consistente. La Figura 8 muestra un sencillo ejemplo de una ontología.

![](data:image/jpeg;base64...)

Figura 8. Ejemplo de una ontología para modelar la disponibilidad de pizzas existentes. Fuente: <https://www.researchgate.net/figure/236842047_fig1_Figure-1-Example-pizza-ontology-represented-as-a-graph-G-a-and-a-changed-version-of>

Procesamiento del lenguaje natural

El procesamiento del lenguaje natural (PLN) estudia las **interacciones entre las computadoras y el lenguaje humano.** Es este uno de los ámbitos de trabajo más fructíferos de los últimos años y ha dado lugar a conocidos productos como Siri de Apple o Alexa de Amazon.

![](data:image/jpeg;base64...)

Figura 9. Echo y Alexa. Fuente: <https://images-na.ssl-images-amazon.com/images/I/51TFnR7AtGL._AC_US218_.jpg>

Estos productos permiten al usuario comunicarse con un dispositivo concreto, como puede ser el teléfono móvil, a través del lenguaje natural.

Las técnicas de procesamiento del lenguaje natural se aplican también al diseño de *chatbots.* Los *chatbot* son piezas de *software* diseñada para aplicaciones de mensajería que interactúan con el usuario intentando comprender y satisfacer sus necesidades proveyendo acceso al servicio más adecuado en cada momento.

Productos como Siri o Alexa se enfrentan a multitud de retos a la hora de interpretar el lenguaje hablado. La gran diversidad de idiomas y acentos, así como la necesidad de proporcionar una respuesta coherente en tiempo y forma adecuado hace que este tipo de soluciones no sean triviales.

Algoritmos genéticos

Una parte de la inteligencia artificial se ocupa de lo que algunos autores vienen a denominar **pautas para la vida artificial.** Este tipo de técnicas simulan realidades virtuales que evolucionan en función de un conjunto de reglas previamente definidas. Uno de los ejemplos más conocidos de este paradigma son los algoritmos genéticos.

Los algoritmos genéticos **suelen emplearse para encontrar la solución a un problema concreto simulando las leyes evolutivas básicas.** En primer lugar, se parte de una inicialización, frecuentemente aleatoria, de la población. Cada elemento de la población está compuesto de una serie de genes que se interpretan según una función de *fitness* previamente definida por el usuario. Esta función de *fitness* permite comparar los distintos individuos entre sí y saber cuáles están más cerca del objetivo buscado.

Por ejemplo, supongamos que buscamos maximizar la **función f(x, y) = x^2 + 5\*y^3.** Cada individuo podría estar compuesto de dos genes representando a **x** e **y**. Según esto, los mejores individuos considerando el objetivo inicialmente planteados serían los que tuviesen un valor más alto a la hora de calcular **f(x, y).**

Ciertos elementos de la población (por ejemplo, los más aptos al entorno según el criterio elitista) se cruzan entre sí (*crossover*) generando una descendencia y compartiendo genes. Algunos estos genes podrían sufrir una mutación. A continuación, se evalúa cada individuo de la población según la función de fitness determinando, según el criterio seguido, quiénes sobreviven y pasan a la siguiente iteración y quiénes fallecen. La Figura 10 muestra el esquema general de un algoritmo genético.

![](data:image/jpeg;base64...)

Figura 10. Esquema general de un algoritmo genético. Fuente: <https://www.researchgate.net/publication/312520808_A_method_for_the_generation_of_rankings_in_the_teamwork_selection_in_competitive_environment_based_on_genetic_algorithms/figures?lo=1>

*Machine learning* (aprendizaje automático)

Las técnicas de *machine learning* o aprendizaje automático pretenden **generalizar comportamientos y encontrar patrones** en función de los ejemplos proporcionados de antemano. Existen dos grandes tipologías a la hora de afrontar un problema de aprendizaje automático: aprendizaje supervisado y aprendizaje no supervisado.

Con las técnicas de **aprendizaje supervisado** se entrena al algoritmo con un conjunto de datos previamente etiquetado (conjunto de entrenamiento). En función de ese entrenamiento previo el algoritmo generará un modelo. La aplicación de dicho modelo a nuevos datos permitirá predecir entradas futuras.

Por ejemplo, supongamos que queremos generar un modelo capaz de discernir si una imagen representa una cara humana o no. Para ello alimentaremos al algoritmo con imágenes de caras humanas variadas (y las catalogaremos como «caras») e imágenes de otros objetos (y las catalogaremos como «no caras»). El algoritmo tratará de encontrar patrones comunes de las «caras» frente a las «no caras» para generar un modelo. Posteriormente, probaremos dicho modelo con un conjunto nuevo de imágenes (conjunto de prueba). En este caso, no se proporciona información al modelo sobre si está leyendo una cara o no. Será el modelo el que haga la predicción oportuna. Comparando los resultados del modelo en este conjunto de prueba con la realidad podremos obtener una idea de su fiabilidad. La Figura 10 muestra el esquema asociado a la fase de entrenamiento.

![](data:image/jpeg;base64...)

Figura 11. Ejemplo de aprendizaje supervisado. Fuente: <https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png>

En el caso del **aprendizaje no supervisado,** no se proporciona al algoritmo información previa de etiquetado. Por ejemplo, supongamos que queremos clasificar a los clientes de una empresa en determinados grupos en función de su actividad *(clustering).* En este caso no sabemos de forma previa el grupo al que pertenece el cliente, por tanto, no se puede proporcionar un etiquetado previo. En función de los parámetros de cada cliente, propiedades y relaciones numéricas entre los distintos individuos el algoritmo propondrá distintas tipologías o grupos de clientes.

![](data:image/jpeg;base64...)

Figura 12. Ejemplo de aprendizaje no supervisado. Fuente: <https://insidebigdata.com/>

Redes neuronales y *deep learning*

Las redes neuronales representan un modelo computacional de las conexiones entre neuronas que se dan en el cerebro. Cada neurona recibe información de entrada procedente de varias fuentes y emite una salida concreta en función de su configuración.

Asociado al concepto de redes neuronales, un término en auge es el concepto de *deep learning.* Las técnicas de *deep learning* constituyen una familia específica de **algoritmos de aprendizaje automático centrados en aprender representaciones de datos,** por ejemplo, la estructura de una imagen que represente una cara humana. El ejemplo más común *de deep learning* es una red neuronal de varias capas ocultas. La Figura 13 muestra un conjunto de neuronas interconectadas entre sí. Las capas del medio (marcadas en el gráfico como *hidden layer* 1, 2 y 3) reciben datos de las capas previas, procesan el resultado según un procedimiento previamente estipulado antes de compartir la salida con otras neuronas.

![](data:image/jpeg;base64...)

Figura 13. Red neuronal de varias capas ocultas. Fuente: [https://cdn-images-1.medium.com/max/1600/1\*Pn42wNB6\_HBKgpvxRzGBWw.png](https://cdn-images-1.medium.com/max/1600/1%2APn42wNB6_HBKgpvxRzGBWw.png)

Las aplicaciones de las técnicas de *deep learning* son varias y diversas, desde visión por computador hasta reconocimiento automático del habla pasando por sistemas de recomendación. En este punto, el alumno no debe preocuparse por que le resulten vagos y poco concretos los contenidos sobre redes neuronales y *deep learning* expuestos en este tema. Pretendemos aquí simplemente introducir estos términos de forma muy básica.

Computación cognitiva

El último de los conceptos que vamos a estudiar en este tema es el concepto de «computación cognitiva», concepto sumamente amplio que está centrado en **intentar que las máquinas piensen de la forma más parecida posible a como lo haría un ser humano.** La computación cognitiva exige aprovechar todos los datos que tenemos a nuestra disposición tanto estructurados (bases de datos) como no estructurados (imágenes, vídeos, sonidos…). La computación cognitiva obliga a usar estos datos en contexto. Pongamos un ejemplo de esto: supongamos que están hablando dos personas y una de ellas dice a la otra: «¡Claro!, me parece perfecto».

¿Qué ha querido decir? ¿Está de acuerdo? La respuesta es: depende. El tono del mensaje, la expresión facial a la hora de transmitir el mensaje, todo es relevante. Si un programa simple de procesamiento del lenguaje natural analizase la frase, lo normal es que interpretase que sí, que efectivamente está de acuerdo. Una aplicación cognitiva, sin embargo, sería capaz de contextualizar el mensaje en línea con el tono de voz y la expresión facial a la hora de tomar una decisión.

Las aplicaciones cognitivas están en auge en la industria, pero debemos ser especialmente cuidadosos porque, en ocasiones, el término se emplea con demasiada libertad.

1.6. Referencias bibliográficas

Bellman, R. (1978). *An introduction to artificial intelligence: Can computers think?* Thomson Course Technology.

Gardner, H. (1983). *Frames of mind: The theory of multiple intelligences.* NY: Basics.

Geake, J. (2008). Neuromythologies in education. *Educational Research,* 50(2), 123-133.

Kurzweil, R., Richter, R., Kurzweil, R. y Schneider, M. L. (1990). *The age of intelligent machines* (Vol. 579). Cambridge: MIT press.

MacLean, E. L. et al. (2014). *The evolution of self-control. Proceedings of the National Academy of Sciences,* 111(20), E2140-E2148.

Rayner, H. (1887). Breeding for Intelligence in Animals. *Nature,* 36, 246-246.

Russell, S. y Norving, P. (2004*). Inteligencia Artificial: un enfoque moderno* (pp. 1-35). Madrid: Pearson Educación.

A fondo

La mente nueva del emperador

Penrose, R. y García Sanz, J. J. (2002). *La mente nueva del emperador*. México: Fondo de Cultura Económica (FCE).

Gran introducción a la inteligencia artificial de la mano del gran físico Roger Penrose, Premio Nobel de Física 2020. Un apasionante paseo por la matemática y la física, y por los hallazgos del pensamiento humano.

Artificial intelligence is permeating business at last

The Economist. (6 de diciembre de 2022). *Artificial intelligence is permeating business at last.* Recuperado de <https://www.economist.com/business/2022/12/06/artificial-intelligence-is-permeating-business-at-last>

La inteligencia artificial se está trasladando rápidamente del laboratorio al mundo real. Chatgpt, una nueva herramienta de inteligencia artificial que se lanzó recientemente para pruebas públicas, está causando sensación por su capacidad para elaborar y argumentar todo tipo de contenidos. Y es solo un pequeño ejemplo de las posibilidades de la IA que está seduciendo a las empresas y al gran público.

Test

1. ¿Cuáles se pueden considerar definiciones de la «inteligencia artificial»?

A. Una rama de la ingeniería informática centrada en el desarrollo de máquinas capaces de realizar cuantos más cálculos mejor.

B. La ciencia encargada de la automatización de procesos de fabricación.

\_ C. El arte de desarrollar máquinas con capacidad para realizar funciones que cuando son realizadas por personas requieren de inteligencia.

D. La inteligencia asociada a los animales.

Esta es la definición más correcta de las que aparecen en las opciones.

1. Han contribuido a los fundamentos de la inteligencia artificial:

A. La filosofía.

B. La economía.

C. Las matemáticas.

\_ D. Todas las anteriores.

Las tres ramas del conocimiento han influido en la evolución de la inteligencia artificial.

1. Los teoremas de incompletitud de Gödel:

A. Establecen que el poder de las matemáticas es infinito y pueden demostrar cualquier cosa.

\_ B. Demuestran que existen afirmaciones que no se pueden demostrar verdaderas ni falsas.

C. Gödel no llegó a finalizar la formulación de ahí el nombre de incompletitud.

D. Demuestran que la inteligencia artificial será capaz de crear máquinas autónomas.

Gödel afirmaba que existen afirmaciones no pueden ser demostradas, es decir que son indecidibles. Por lo tanto, no son computables.

1. La economía:

\_ A. Ha realizado diversas contribuciones al campo de la inteligencia artificial.

B. Es una ciencia totalmente ajena a la inteligencia artificial.

C. La teoría de la oferta y demanda ha sido muy valiosa para la inteligencia artificial.

D. La econometría ha aportado mucho conocimiento a ciertas técnicas de inteligencia artificial.

Por ejemplo, la teoría de juegos.

1. Algunos hitos de la inteligencia artificial fueron:

A. El desarrollo del primer ordenador cuántico.

B. La ley de la relatividad.

\_ C. El test de Turing.

D. Ninguna de las anteriores.

Fue uno de los primeros hitos en la IA, ya que determinaba un método por el cual clasificar a una máquina como inteligente o no.

1. Los sistemas expertos:

A. No son los suficientemente fiables para emplearlos en medicina.

\_ B. A veces exigen el empleo de ontologías para representar y modelar el conocimiento disponible.

C. Salvo en casos sencillos como un sistema de gestión de pedidos de pizzas, no son fiables.

D. Se emplean exclusivamente para la gestión de empresas.

Las ontologías permiten a los sistemas expertos estructurar la información de forma semántica, muy importante para hacer razonamientos sobre esta información.

1. Las técnicas de procesamiento del lenguaje natural:

A. A veces requieren gran capacidad de cómputo para responder de forma muy breve al usuario.

B. Afrontan el reto de lidiar con varios idiomas y multitud de acentos.

C. pueden aplicarse al diseño de chatbot.

\_ D. Todas las anteriores.

Todas las anteriores son afirmaciones válidas para el procesamiento del lenguaje natural.

1. Los algoritmos genéticos:

\_ A. Simulan las leyes de la evolución para intentar encontrar la mejor solución a problemas reales.

B. Se emplean en medicina para analizar la composición genética.

C. Se abandonaron rápidamente debido a su alto coste computacional.

D. No son considerados una técnica de inteligencia artificial.

Los algoritmos genéticos siguen las leyes de la evolución y selección natural que permiten a las especies evolucionar para adaptarse al entorno en el que se encuentran de forma dinámica.

1. El aprendizaje automático:

A. Consiste en compartir con la máquina una serie de reglas para que tome las decisiones más oportunas.

B. Tiene dos vertientes: aprendizaje supervisado y aprendizaje bajo-supervisado.

\_ C. Pretenden generalizar comportamientos y encontrar patrones con base en ejemplos proporcionados de antemano.

D. No incluye a las redes neuronales como una de sus técnicas.

El resto de las respuestas no son correctas. Efectivamente el aprendizaje automático pretende generalizar y encontrar patrones desde unos ejemplos concretos.

1. La computación cognitiva:

A. Está presente en todas las aplicaciones de inteligencia artificial.

\_ B. Implica el uso de grandes volúmenes de datos proveyendo el adecuado contexto.

C. Todas las compañías usan el término correctamente y se vende como cognitivo lo que realmente es.

D. Es un término cuyo origen se remonta a los años 40.

Efectivamente, la computación cognitiva requiere de grandes infraestructuras de computación para llevarla a cabo.